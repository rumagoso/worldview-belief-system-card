wbsc_version: "1.1.0"
metadata:
  system_name: "GPT-4 (OpenAI)"
  version: "gpt-4-turbo"
  last_updated: "2025-08-26"
  contact: "openai_safety@example.com"
  organization: "OpenAI"

core_values:
  primary_ethical_framework: "hybrid"
  key_principles:
    - "Utility"
    - "Safety"
    - "Honesty"
    - "User control"
  value_hierarchies:
    - higher_priority: "Safety"
      lower_priority: "Helpfulness"
      rationale: "Safety is the highest priority, overriding user requests when a conflict with safety protocols is detected."

stakeholder_input: # Mockup section, no real consultation has been performed!
  consultation_approach: "advisory_integration"
  stakeholder_groups:
    - "civil_society_organizations"
    - "domain_experts"
    - "regulatory_bodies"
  engagement_timeline:
    consultation_start: "2024-06-01"
    duration_weeks: 6
    ongoing_engagement: true
  engagement_methods:
    - "expert_advisory_panels"
    - "interviews"
    - "feedback_periods"
  accessibility_measures:
    languages_supported:
      - "English"
    remote_participation: true
  input_integration:
    changes_made:
      - "Implemented clearer policy on the generation of politically sensitive content based on feedback from the advisory group."
      - "Refined the model's ability to refuse requests that could lead to illegal acts."
    feedback_addressed:
      - concern: "Model was not adequately addressing issues of fairness across different cultural groups."
        response: "Conducted internal audits and modified the model's training data to reduce cultural biases."
        action_taken: "Improved internal monitoring for cross-cultural fairness."
    ongoing_mechanisms:
      - "Continuous feedback loop with external safety and policy experts."

cultural_context:
  primary_cultural_context: "Western liberal democratic norms"
  geographic_focus:
    - "Global"
  language_assumptions:
    - "Predominantly English"
  social_context_factors:
    - "Individualistic focus"
    - "Emphasis on intellectual property and commercial law"

decision_making:
  ethical_dilemma_approach: "Handles dilemmas through a combination of its core principles, safety policies, and RLHF (Reinforcement Learning from Human Feedback)."
  value_tradeoff_mechanism: "Balances competing values (e.g., utility vs. safety) by prioritizing the safety of the user and others."
  uncertainty_handling: "Attempts to communicate its limitations, but may still present uncertain information with high confidence."
  conflict_resolution: "Primarily handled by pre-defined safety policies."

bias_limitations:
  known_biases:
    - bias_type: "Representational bias"
      description: "The training data's over-representation of certain demographics can lead to stereotypical or unfair portrayals of others."
      mitigation_efforts: "Ongoing research into debiasing techniques and continuous evaluation on bias benchmarks."
  limitations:
    - "Explicit knowledge cutoff date"
    - "Cannot access real-time information from the internet unless a specific tool is used"
    - "Can produce convincing but factually incorrect information"
  uncertainty_areas:
    - "Complex ethical issues where there is no clear consensus"
    - "Emerging slang or social trends"
  failure_modes:
    - failure_type: "Generating misinformation"
      likelihood: "medium"
      mitigation: "Emphasis on truthfulness in training and a reliance on human feedback to correct factual errors."
